# -*- coding: utf-8 -*-
"""TFM_bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cMIh1S2Z00JG8FcVlePCIZwK1HXWEEnF
"""

!pip install transformers datasets torch
import torch
from datasets import Dataset
from google.colab import drive
import ast
import pandas as pd
import torch
import numpy as np
from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments
from datasets import Dataset, DatasetDict
from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support, classification_report
from transformers import AutoModelForTokenClassification, AutoTokenizer
from torch.nn.functional import softmax

drive.mount('/content/drive')
tfmPath = '/content/drive/MyDrive/TFM'
trainingFilesPath = tfmPath+'/archivos_entrenamiento'
validationFilePath = tfmPath+'/archivos_validacion'
testFilePath = tfmPath+'/archivo_test'
model_path = tfmPath+'/modeloBert'

archivo = trainingFilesPath+'/train.txt'

with open(archivo, 'r', encoding='utf-8') as file:
    content = file.read()
    data = ast.literal_eval(content)

print(data)

archivo_val = validationFilePath+'/val.txt'

with open(archivo_val, 'r', encoding='utf-8') as file:
    content = file.read()
    data_val = ast.literal_eval(content)

print(data_val)

archivo_test = testFilePath+'/test.txt'

with open(archivo_test, 'r', encoding='utf-8') as file:
    content = file.read()
    data_test = ast.literal_eval(content)

print(data_test)

# Preprocesamiento de los datos

tokenizer = BertTokenizerFast.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')

def preprocess_data(data):
    tokenized_data = []

    for text, entity_dict in data:
        entities = entity_dict['entities']

        # Se tokeniza el texto
        tokenized_inputs = tokenizer(text, truncation=True, padding='max_length', max_length=128, is_split_into_words=False)

        # Se inicializa la lista de etiquetas con -100 para ignorar
        labels = [-100] * len(tokenized_inputs["input_ids"])

        # Se asignan etiquetas a los tokens de las entidades
        for start, end, entity_label in entities:
            # Se tokeniza la parte del texto que corresponde a la entidad
            entity_tokens = tokenizer.encode(text[start:end], add_special_tokens=False)

            # Se busca la posición de los tokens de la entidad en `tokenized_inputs`
            for idx in range(len(tokenized_inputs["input_ids"])):
                # Se verifica si el segmento de tokens coincide
                if tokenized_inputs["input_ids"][idx:idx + len(entity_tokens)] == entity_tokens:
                    entity_id = label2id[entity_label]
                    for i in range(len(entity_tokens)):
                        labels[idx + i] = entity_id  # Se asigna la etiqueta a todos los subtokens
                    break

        # Se añaden las etiquetas a la entrada tokenizada
        tokenized_inputs["labels"] = labels
        tokenized_data.append(tokenized_inputs)

    return tokenized_data

#Configuración del Modelo BERT para NER

# Se recopilan las etiquetas únicas de los datos
unique_labels = set()
for _, entity_dict in data:
    for _, _, label in entity_dict['entities']:
        unique_labels.add(label)

# Se converte el conjunto en una lista ordenada para asegurar consistencia
label_list = sorted(list(unique_labels))

# Se crea un id2label y un label2id dinámicamente
id2label = {i: label for i, label in enumerate(label_list)}
label2id = {label: i for i, label in id2label.items()}

# Se imprimen los resultados para verificar
print("id2label:", id2label)
print("label2id:", label2id)

# Se carga el modelo BERT
model = BertForTokenClassification.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', num_labels=len(id2label), id2label=id2label, label2id=label2id)

# Se configura el entrenamiento
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    logging_steps=10,
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.001,
    report_to="none"
)

#Datos de train
df = preprocess_data(data)

# Convertimos a Dataset de Hugging Face
df = pd.DataFrame(df)
df_subset = df.iloc[:2136]
dataset_train = Dataset.from_pandas(df_subset)

#Se hace un print para verificar si está en el formato adecuado
print(type(dataset_train))

#Datos de validation

df_val = preprocess_data(data_val)

# Convertimos a Dataset de Hugging Face
df_val = pd.DataFrame(df_val)
df_val_subset = df.iloc[:403]
dataset_val = Dataset.from_pandas(df_val_subset)

#Se hace un print para verificar si está en el formato adecuado
print(type(dataset_val))

#Datos de train
df_test = preprocess_data(data_test)

# Convertimos a Dataset de Hugging Face
df_test = pd.DataFrame(df_test)
df_subset_test = df_test.iloc[:443]
dataset_test = Dataset.from_pandas(df_subset_test)

dataset = DatasetDict({
    "train": dataset_train,
    "validation": dataset_val,
    "test": dataset_test
})

#Se hace un print para verificar si está en el formato adecuado
print(dataset)

# Entrenamiento

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
)

trainer.train()
results = trainer.evaluate()
print(results)

# Se guardar el modelo entrenado en un directorio
trainer.save_model(tfmPath+'/modeloBert')

# Guardar el modelo y el tokenizador
model.save_pretrained(tfmPath+'/modeloBert')
tokenizer.save_pretrained(tfmPath+'/modeloBert')

# Cargar el modelo y el tokenizador
model = AutoModelForTokenClassification.from_pretrained(tfmPath+'/modeloBert')
tokenizer = AutoTokenizer.from_pretrained(tfmPath+'/modeloBert')

# Se extraen las etiquetas del modelo
id2label = model.config.id2label  # Diccionario {id: etiqueta}
label2id = model.config.label2id  # Diccionario {etiqueta: id}

# Se converte el id2label a lista
label_list = [label for _, label in sorted(id2label.items())]
print(label_list)

# Se evalua el modelo con dato previamente no vistos
def evaluar_modelo(model, test_dataset):

    y_true = []
    y_pred = []

    #Se itera sobre los datos del conjunto de prueba
    for example in test_dataset:
        # Se preparan las entradas para el modelo
        inputs = {
            "input_ids": torch.tensor([example['input_ids']]),
            "attention_mask": torch.tensor([example['attention_mask']])
        }
        if "token_type_ids" in example:
            inputs["token_type_ids"] = torch.tensor([example["token_type_ids"]])

        # Se obtenen las etiquetas reales
        labels = example['labels']

        # Se realiza la predicción
        with torch.no_grad():
            outputs = model(**inputs)
        logits = outputs.logits
        predictions = logits.argmax(dim=-1).squeeze().tolist()

        # Se alinear etiquetas reales con las predicciones (ignorar padding)
        aligned_labels = []
        aligned_predictions = []
        for label, prediction in zip(labels, predictions):
            if label != -100:  # Ignorar tokens de padding
                aligned_labels.append(label)
                aligned_predictions.append(prediction)

        y_true.extend(aligned_labels)
        y_pred.extend(aligned_predictions)

    #Se calcular métricas globales
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='weighted', labels=np.unique(y_true)
    )
    f1_micro = precision_recall_fscore_support(y_true, y_pred, average='micro')[2]
    f1_macro = precision_recall_fscore_support(y_true, y_pred, average='macro')[2]

    #Se crea un informe detallado
    reporte = classification_report(y_true, y_pred, digits=4, zero_division=0)

    #Se imprimir el informe y métricas
    print("=== Informe ===")
    print(reporte)
    print(f"F1 Micro: {f1_micro:.4f}")
    print(f"F1 Macro: {f1_macro:.4f}")

    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "f1_micro": f1_micro,
        "f1_macro": f1_macro,
        "reporte": reporte
    }

evaluar_modelo(model, dataset_test)

# Se pide un texto al importar al usuario
texto_input = input("Por favor, introduce el texto: ")

# Se tokeniza el texto importado

def preprocess_text(texto_input, tokenizer):
    tokens = tokenizer(texto_input, return_tensors="pt", truncation=True, padding=True)
    return tokens

# Se utiliza el modelo preentrando para hacer predicciones sobre las etiquetas de los tokens

def predict_entities(text, model, tokenizer, id2label):
    model.eval()  # Poner el modelo en modo evaluación

    # Se tokeniza el texto
    tokens = preprocess_text(text, tokenizer)

    # Se obtienen las predicciones del modelo
    with torch.no_grad():
        outputs = model(**tokens)
        logits = outputs.logits
    # Se calculan las probabilidades
    probabilities = softmax(logits, dim=-1)

    # Se define umbral
    confidence_threshold = 0.99

    # Se filtran las predicciones
    predictions = []
    confidences = []

    for token_probs in probabilities.squeeze(0):  # Iterar sobre tokens
        max_prob, predicted_class = torch.max(token_probs, dim=-1)
        if max_prob >= confidence_threshold:
            predictions.append(predicted_class.item())
            confidences.append(max_prob.item())
        else:
            predictions.append(None)
            confidences.append(None)

    # Se filtra con la attention mask
    attention_mask = tokens['attention_mask']
    filtered_predictions = [pred if mask == 1 else None for pred, mask in zip(predictions, attention_mask.squeeze().tolist())]

    # Se convierten los tokens en texto legible
    tokens_list = tokenizer.convert_ids_to_tokens(tokens["input_ids"].squeeze().tolist())

    # Se alinean las etiquetas ignorando tokens irrelevantes
    aligned_preds = []
    aligned_tokens = []

    for token, pred, mask in zip(tokens_list, filtered_predictions, tokens["attention_mask"].squeeze().tolist()):
        if mask == 1 and token not in ["[CLS]", "[SEP]"] and pred != None:
            aligned_preds.append(id2label.get(pred, "O"))  # Default "O" si predicción no está en id2label
            aligned_tokens.append(token)

    # Se crean la lista final de resultados
    result = list(zip(aligned_tokens, aligned_preds))

    return result

result = predict_entities(texto_input, model, tokenizer, id2label)

# Se muestran resultados
for token, label in result:
    print(f"{token}: {label}")

def join_subtokens_with_labels(result):
    # Lista para almacenar los resultados en el formato deseado
    joined_result = []

    # La primera palabra no tiene "##", así que se agrega directamente
    current_token, current_label = result[0]

    # Iteramos sobre los tokens y sus etiquetas
    for token, label in result[1:]:
        if token.startswith("##"):
            # Si el token empieza con "##", se une al anterior
            current_token += token[2:]
        else:
            joined_result.append((current_token, current_label))
            current_token, current_label = token, label

    # Se añade el último token y etiqueta
    joined_result.append((current_token, current_label))

    return joined_result

# Se unen los subtokens y las etiquetas
joined_result = join_subtokens_with_labels(result)

# Se imprimen los resultados
for token, label in joined_result:
    print(f"Token: {token}, Label: {label}")

for token, label in result:
  if label == "gen" and label == "gen":
    print(f"Considere cambiar 'los alumnos' por 'el alumnado'. ")
  if label == "profs":
    print(f"Considere cambiar 'los profesores' por 'el profesorado'. ")
  if label == "inv":
    print(f"Considere cambiar 'los investigadores' por 'el equipo de investigación'. ")
  if label == "hab":
    print(f"Considere cambiar 'los habitantes' por 'la población'. ")
  if label == "cam":
    print(f"Considere cambiar 'los camareros' por 'el personal de servicio'. ")
  if label == "ing":
    print(f"Considere cambiar 'los ingenieríeros' por 'el equipo de ingeniería'. ")
  if label == "bus":
    print(f"Considere cambiar 'los hombres de negocios' por 'las personas de negocios'. ")
  if label == "cl":
    print(f"Considere cambiar 'los clientes' por 'la clientela' ")
  if label == "dis1":
    print(f"Considere cambiar 'el discapacitado' por '(persona) con discapacidad'.")
  if label == "dis2":
    print(f"Considere cambiar 'el minusválido' por '(persona) con discapacidad'.")
  if label == "dis3":
    print(f"Considere cambiar 'el inválido' por '(persona) con discapacidad'.")
  if label == "dis4":
    print(f"Considere cambiar 'el disminuido' por '(persona) con discapacidad'.")
  if label == "dis5":
    print(f"Considere cambiar 'el retrasado' por '(persona) con discapacidad'.")
  if label == "dis6":
    print(f"Considere cambiar 'el tetrapléjico'  por 'la persona con tetrapléjia'.")
  if label == "vdis1":
    print(f"Considere cambiar 'sufir discapacidad' por 'tener discapacidad' ")
  if label == "vdis2":
    print(f"Considere cambiar 'padecer discapacidad' por 'tener discapacidad' ")
  if label == "":
    print(f"(No se han encontrado mejoras según los criterios establecidos.")

if len(doc.ents) == 0:
  print(f"No se han encontrado mejoras según los criterios establecidos.")

for ent in doc.ents:
    if ent.label_ == "gen" and "alumnos" in ent.text:
        print(f"Considere cambiar {ent.text} por 'el alumnado'. ")
    if ent.label_ == "gen" and "profesores" in ent.text:
        print(f"Considere cambiar {ent.text} por 'el profesorado'. ")
    if ent.label_ == "gen" and "investigadores" in ent.text:
        print(f"Considere cambiar {ent.text} por 'el equipo de investigación'. ")
    if ent.label_ == "gen" and "habitantes" in ent.text:
      print(f"Considere cambiar {ent.text} por 'la población'. ")
    if ent.label_ == "gen" and "camareros" in ent.text:
      print(f"Considere cambiar {ent.text} por 'el personal de servicio'. ")
    if ent.label_ == "gen" and "ingenieros" in ent.text:
      print(f"Considere cambiar {ent.text} por 'el equipo de ingeniería'. ")
    if ent.label_ == "gen" and "hombres de negocios" in ent.text:
      print(f"Considere cambiar {ent.text} por 'las personas de negocios'. ")
    if ent.label_ == "gen" and "clientes" in ent.text:
      print(f"Considere cambiar {ent.text} por 'la clientela' ")


    if ent.label_ == "dis" and "discapacitad" in ent.text:
      print(f"Considere cambiar {ent.text} por '(persona) con discapacidad'.")
    if ent.label_ == "dis" and "invalid" in ent.text:
      print(f"Considere cambiar {ent.text} por '(persona) con discapacidad'.")
    if ent.label_ == "dis" and "misunsvalid" in ent.text:
      print(f"Considere cambiar {ent.text} por '(persona) con discapacidad'.")
    if ent.label_ == "dis" and "disminuid" in ent.text:
      print(f"Considere cambiar {ent.text} por '(persona) con discapacidad'.")
    if ent.label_ == "dis" and "retrasad" in ent.text:
      print(f"Considere cambiar {ent.text} por '(persona) con discapacidad'.")
    if ent.label_ == "dis" and "tetrapléjic" in ent.text:
      print(f"Considere cambiar {ent.text} por '(persona) con tetrapléjia'.")
    if ent.label_ == "dis" and "sufr" in ent.text:
      print(f"Considere cambiar {ent.text} por 'tener discapacidad'.")
    if ent.label_ == "dis" and "padec" in ent.text:
      print(f"Considere cambiar {ent.text} por 'tener discapacidad'.")